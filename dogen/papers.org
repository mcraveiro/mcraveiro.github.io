#+title: MDE Papers
#+author: Marco Craveiro
#+options: author:nil
#+bind: org-html-validation-link nil
#+HTML_HEAD: <link rel="stylesheet" href="../css/tufte.css" type="text/css" />

[[file:../index.org][Back to home page]].

Listing of papers I have read, and the notes I made. Most of the notes
use [[https://github.com/jkitchin/org-ref][org-ref]] so that I can refer back to the page where they were made.

* Systems variability modeling : a textual model mixing class and feature concepts
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/Systems Variability Modeling: A Textual Model Mixing Class and Feature Concepts.pdf
  :END:

- Younis, Ola, Said Ghoul, and Mohammad H. Alomari.
- "Systems variability modeling: a textual model mixing class and
  feature concepts."
- [[https://arxiv.org/abs/1311.3243][arXiv preprint arXiv:1311.3243 (2013)]].

** Variability: separated approach
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.37154989384288745)
   :END:

The approach of having variability separate from regular modeling is
called the "separated approach".

** Textual notation and mixing
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.41401273885350315)
   :END:

Author claims that textual notations do not provide "integrated"
management of both modeling and variability, e.g. follow the separated
approach.

The opposite of the separate approach is "mixing", where we mix
classes and features.

** Features as conceptual elements
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.41401273885350315)
   :END:

In the author's view features are conceptual elements whereas regular
modeling is physical. We take the view that regular modeling is a
conceptual concept and the physical layer lies in the realm of files
and directories.

** Product line steps
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.6794055201698513)
   :END:

The idea that there are several steps in the the design and
implementation of a software product. The smaller the number of steps
the better. We should make clear what these steps are in MASD.

** Core assets
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.6900212314225053)
   :END:

From a MASD perspective, the catalog of schematic and repetitive
patterns is the set of reusable core assets, not the meta-model
elements. We need to make this clear in the conceptual model.

** Feature model use in MASD
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.7749469214437367)
   :END:

Unlike regular MDE approaches, in MASD we do not allow users to create
their own feature models for their particular problem domain (what the
author calls the domain space). Instead, users can make /selections/
over the MASD feature model, defining what features are required for
the product in question. However, users cannot redefine the feature
model, only instantiate it.

We should make sure we explain that there are two different views on
variability, depending on whether we are Dogen Developers or Dogen
Users.

** Mixing
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.21231422505307854)
   :END:

Author seems to claim that having a feature model and a class model
separately, and subsequently mixing the two to obtain the SPL product
is still part of the mixed approach. There are several techniques to
perform the mixing:

- constraint additions
- relation definition
- reference links

** Feature selection
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.24416135881104034)
   :END:

The process of feature selection involves resolving constraints
amongst features. Interesting term "fixed features". This seems to
describe the MASD approach well.

** Complexity in configurations step
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.5307855626326964)
   :END:

In proper SPL products that make use of feature models, the mixing of
feature models and class diagrams creates a lot of complexity. In
particular, the selection of features and its impact on the class
models is very important. If the process for mixing is too complex,
users will not be able to make use of this approach. MASD takes a very
simplistic view, defining a very simple feature model in order to
tackle this problem.

** Bridge between product line design and OO
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.6369426751592356)
   :END:

MASD provides a bridge between these two distinct worlds, but the
bridge is very narrow in order to keep the complexity down. The MASD
model has a small number of concepts:

#+begin_quote
Provide a new concise and rich textual notation for feature modeling
and class modeling. It allows simple and natural new way of mixing
feature models and class models using small number of concepts.
#+end_quote

Requirements for a solution:

- the mixing solution should be both concise and rich.
- simple, coherent, and complete configuration generation

** CLAFER mixing
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.8174097664543524)
   :END:

The approach used by CLAFER for their mixing is interesting. It is
based on constraints and inheritance. The FM is a collection of type
definitions and features and it is joined with the class model as
attributes and attribute values. We need to read up on CLAFER.

** OOFM: Sarinho and Apolonario
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.2653927813163482)
   :END:

Interesting paper to read: V. T. Sarinho and A. L. Apolinario (2010),
“Combining feature modeling and Object Oriented concepts to manage the
software variability”, IEEE International Conference on Information
Reuse and Integration (IRI), pp. 344-349.

** ECORE feature model
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.4564755838641189)
   :END:

M. Stephan and M. Antkiewicz (2008), “Ecore.fmp: A tool for editing
and instantiating class models as feature models”, University of
Waterloo, Waterloo, Technical Report.

** Implementation models
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.6581740976645435)
   :END:

MASD focuses solely on implementation models.

** Conceptual model components
   :PROPERTIES:
   :NOTER_PAGE: (5 . 0.13800424628450106)
   :END:

- feature concepts;
- object-oriented concepts;
- rules for mixing classes and features.

** TDM components
   :PROPERTIES:
   :NOTER_PAGE: (5 . 0.5414012738853503)
   :END:

Composed of four feature modules:

- Features types: "This feature module captures all features in the
  system with their possible values. It is composed by Features_Types
  and Relation_Types."
- Features Global: "This feature module specifies the Global features
  that will be shared between all system components."
- Features Control: "This feature module is composed by relations only,
  and its main goal is to keep systems’ components stable and avoid
  any conflicts."
- Features Configuration: "This feature module is composed by
  relations only, and its main goal is to keep systems’ components
  stable and avoid any conflicts."

** Meta-Features Model
   :PROPERTIES:
   :NOTER_PAGE: (5 . 0.6263269639065817)
   :END:

Models all features in TDM, template for feaures in the Features
Meta-Model. Seems like a Meta-Meta-Model.

** Features Meta-Model
   :PROPERTIES:
   :NOTER_PAGE: (6 . 0.16985138004246284)
   :END:

Based on the MFM. "This is an intermediate model between the
conceptual part (Feature Meta-Model) and the physical part (Product
Model).

** Application Areas
   :PROPERTIES:
   :NOTER_PAGE: (11 . 0.25477707006369427)
   :END:

Provide a list of specific use cases that can benefit from MASD with
examples.

** TDM has strong typing
   :PROPERTIES:
   :NOTER_PAGE: (11 . 0.5095541401273885)
   :END:

The TDM approach of having strong types for features is similar to our
approach in that the features meta-model defines the domain of
instantiation for all types used in configuration.

** TDM methodology
   :PROPERTIES:
   :NOTER_PAGE: (11 . 0.7112526539278131)
   :END:

TDM uses feature modeling as part of a wider methodology, guiding its
use. Similarly with MASD, feature modeling is fixed by the methodology
itself, and therefore integrated in the modeling approach.

** Conclusions

Steps:

- from a feature meta-model, we instantiate it by creating features.
-

- features may be of different types. In our case we seem to have
  control features (enablement)


Notes:

- instead of their notion of global features (shared between system
  components) we have features with different binding points: global,
  element, etc.
- we do have a notion of "feature types": we have features that are
  just "input parameters" with values that are read out and used;
  other features such as enablement, produce wider changes.
- we need to explain how our mixing approach fits in with existing
  mixing approaches. Is there a survey of mixing? We need to define a
  methodology for the mixing process.
- the logical model is an OO model. The physical model defines a
  meta-model for the implementation. The variability model defines a
  feature meta-model and model. The product models are instances of
  all of these models.
- we use features for the solution space.
- for us "variant products" has a different meaning. Since we only
  care about SRAPs, to call a MASD product a variant is not very
  meaningful. However, users define families and those have products
  as instances.
- we don't yet solve constraints amongst features. OR: we take an
  empirical approach to features: general statements are hard and
  inflexible so we check for correctness on a case by case basis. This
  is our trade-off.
- our approach is to create a code generation meta-model which exposes
  the smallest amount of MDE concepts to the end user. The end user is
  free to exploit them via existing tooling such as EMF. Our code
  generator is a bridge between regular development and MDE.
- feature models typically focus on problem space. We use them for the
  solution space.
- major disagreement with author: we do not believe text notation is
  better or worse than graphical notation; they have different
  properties and therefore different uses. We'd like to support both.
- taxonomy of features: static, dynamic (feature toggles). For dynamic
  features we need a separate library. Actually, could we use a
  library that handles both? Maybe its just a coincidence that the
  code generator supports its features directly from a model. The
  difference is in the model we have context (global and local
  features).

Summary:

| Aspect              | TDM Approach | MASD Approach |
|---------------------+--------------+---------------|
| Meta-Features model |              | Hard-coded.   |
|                     |              |               |

* A Code Generation Metamodel for ULF-Ware Generating Code for SDL and Interfacing with the Runtime Library
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/A Code Generation Metamodel for ULF-Ware.pdf
  :END:

** Intermediate model
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.40707964601769914)
   :END:

Instead of creating a meta-model that is general and useful for many
use cases, its best to create a meta-model specific for code
generation only.

** Generality and Behaviour
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.45132743362831856)
   :END:

Authors believe that the model should be general in order to support
several target languages, which we agree with, but they also think
behaviour is important - which we disagree.

** Need for behaviour
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.5232558139534883)
   :END:

Authors claim that a structural-only model is not sufficient, but do
not provide additional justification for their views.

** Decreasing complexity by shared meta-model
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.6191860465116279)
   :END:

Not clear why authors think we need n * m libraries. But at any rate,
with a shared infrastructure we can manage to significantly reduce
code duplication. We still need language specific PDMs.

** MOF
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.5319767441860465)
   :END:

The use of a standard meta-meta-model provides a lot of tooling
infrastructure and simplifies the work. We need to justify why we did
not build upon this approach.

** Difficulties in generation
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.6802325581395349)
   :END:

Authors explain the difficulty of code generation from abstract
models. We should mention our solution to this problem, which is to
capture schematic and repetitive patterns found in code.

** Model abstraction level is key
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.19186046511627905)
   :END:

Finding the right level of abstraction at which to model is very
important. If a model is too close to source code it is very difficult
to handle in terms of transformations, if it is too far away from
source code, the code generation is very hard.

** Commonalities for OO
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.313953488372093)
   :END:

Authors make a case for a common model for several OO languages for
code generation.

** Multi-language and common denominators
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.45348837209302323)
   :END:


By supporting multiple languages, the authors decided that only
features that are common to all languages are supported in the
meta-model. Our approach is to have some features supported on some
languages but not others.

** Low-level approach
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.6976744186046512)
   :END:

Authors seem to be concerned with low-level language details at code
generation level. We believe instead that much of the massaging
required can be done at the M2T transformation level, and the role of
the PDMs is only when large missing features need to be provided.

** Too much detail
   :PROPERTIES:
   :NOTER_PAGE: (5 . 0.16569767441860464)
   :END:

    As with the right level of modeling, the right level of detail on the
code generator is also an engineering trade off. We decided to add
more detail to the generation in order to keep the meta-model simpler.

Creating a library to make the different OO languages look the same
has a lot of disadvantages:

- a lot of code to maintain.
- non-idiomatic code that developers of that language will not
  understand.

** UML Infrastructure
   :PROPERTIES:
   :NOTER_PAGE: (5 . 0.7936046511627907)
   :END:

Whilst the authors claim their meta-model is designed for
code-generation, it seems its more concerned with UML compatibility
rather than a close fit to the the requirements of the generator. We
take the exact opposite approach: our model is designed specifically
to fit the code generation requirements.

| CeeJay type | Dogen Type | Comments                                                                          |
|-------------+------------+-----------------------------------------------------------------------------------|
| Package     | Module     | Low-level mapping, forcing all of C++ elements to be declared in the same header. |
| Class       | Object     | No multiple inheritance.                                                          |
|   Types       | N/A        | We use multiple meta-model types for this concept.                                |

** Handling of primitives
   :PROPERTIES:
   :NOTER_PAGE: (7 . 0.19186046511627905)
   :END:

CeeJay expects the existence of a number of "core" types. This is not
enforced formally. Dogen sees these as plain model types, defined in
PDMs. Builtins are no different from any model type.

** Uniform interfaces to collections
   :PROPERTIES:
   :NOTER_PAGE: (7 . 0.27906976744186046)
   :END:

CeeJay's approach of having uniform interfaces to collections is not
ideal because we either duplicate the existing standard libraries into
a "CeeJay standard library", which is not idiomatic, not well
maintained, etc - or worse, we create a small number of simple
collections which do not have the required expressive power of a full
standard library.

** Function modeling
   :PROPERTIES:
   :NOTER_PAGE: (7 . 0.4883720930232558)
   :END:

It is very difficult to model functions (behaviour in
general). Authors point out challenges. We avoid this problem by not
modeling operations other than for the purposes of merging code
generation, which will be added later on.

** Predefined issues
   :PROPERTIES:
   :NOTER_PAGE: (9 . 0.41860465116279066)
   :END:

By relying on MOF, we start to pull in runtime dependencies to the
modeling environment, meaning all users of the generated code are now
exposed to the modeling implementation.

** References
   :PROPERTIES:
   :NOTER_PAGE: (10 . 0.3226744186046511)
   :END:

Papers to read:

Piefel, M.: A common metamodel for code generation. In: J. Aguilar (ed.), Pro-
ceedings of the 3rd International Conference on Cybernetics and Information Tech-
nologies, Systems and Applications. I I I S, Orlando, USA (2006)

* A Lightweight MDSD Process Applied in Small Projects
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/A Lightweight MDSD Process Applied in Small Projects.pdf
  :END:

** Important success factors
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.26519337016574585)
   :END:

- "Its learning curve"
- "its adaptability to special requirements"
- "the maturity of the supporting tools"

** Range of tooling in MDE is vast
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.2900552486187845)
   :END:

Its never quite clear how one decides if a tool is part of MDE or not.

** pragmatic approach
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.2983425414364641)
   :END:

Similar to MDSD, the authors decided to take on a pragmatic approach
with the following key factors:

- partial usage
- create a tool rather than use an existing one due to cost factors;
  the include open source tools as well.
- iterative approach to building the tool.

** Target is small projects
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.3314917127071823)
   :END:

Similar to MASD, the target of this paper is the small and middle size
projects with limited resourcing.

** Requirments
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.2154696132596685)
   :END:

The requirements for this project are actually quite general:

- risk mitigation by allowing the use of traditional development if
  MDE is found to be problematic.
- no extra time allowed to develop MDE tools.
- cost is an important factor and training is limited.
- code should be human readable and modifiable.

** Activities
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.505524861878453)
   :END:

The approach is a traditional dual-track development model, with a
separation of the MDE team from the main team, and using integration
points to join the work. MASD is very different in that we intend to
back out the MDE parts of the work by empirical analysis of existing
code.

** MDE develppment process
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.7790055248618785)
   :END:

Crucially, the MDE development process is driven by the MDE team. They
work off of prototypes they develop and then deliver features to the
agile team which provides feedback. The problem with this approach is
that the features do not need to be grounded on the needs of the agile
team and can be speculative.

** Template development
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.09944751381215469)
   :END:

It is positive that templates are attempting to recreate code that was
manually crafted, as this is a very good sanity check.

** Bibliography
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.09944751381215469)
   :END:


- “Code Generation Information for the Pragmatic Engineer,” http://www.codegeneration.net/.
- V. Kulkarni and S. Reddy, “Introducing MDA in a Large IT Consultancy
  Organization,” in APSEC. IEEE Computer Society, 2006, pp. 419–426.
- G. Guta, B. Szasz, and W. Schreiner, “A Lightweight Model Driven Development Process based on XML Technology,”
RISC Report Series, University of Linz, Austria, Tech. Rep. 08-01, March 2008.

* Proceso de Desarrollo de Software Mediante Herramientas MDA

Link: http://www.iiisci.org/journal/CV$/risci/pdfs/C476AI.pdf

* Un estudio comparativo de dos herramientas MDA: OptimalJ y ArcStyler
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/Un estudio comparativo de dos herramientas MDA: OptimalJ y ArcStyler.pdf
  :END:

Link: http://dis.um.es/~mjortin/articulos/tdsdm04.pdf

Words that are not clear:

- trazabilidad
- marcas: tagging?

** Important factors
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.46060254924681343)
   :END:

- PIM annotations
- whether the PSM is implicit or explicit.

** MDA principles
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.6170336037079954)
   :END:

- split the specification of the functionality of a software system
  from the implementation.

** Model compiler
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.7734646581691773)
   :END:


MDA focuses on the creation of "model compilers" but we have not yet
found a formal definition of what one is. It is clearly related to the
transformations from CIM, PIM, PSM etc.

** Evaluation properties
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.6344148319814601)
   :END:

Properties that are evaluated for each tool. We should evaluate Dogen
against these as well. Measured as 0 to 4: nulo, mínimo, medio, bueno,
excelente

| Id  | Property                             | Comment                |
|-----+--------------------------------------+------------------------|
| P01 | PIM support                          |                        |
| P02 | PSM support                          |                        |
| P03 | Multiple implementations.            |                        |
| P04 | Model integration                    |                        |
| P05 | Interoperability                     |                        |
| P06 | Transformation definition            |                        |
| P07 | Model verification                   |                        |
| P08 | Use of patterns (GoF)                |                        |
| P10 | Support for incremental consistency  |                        |
| P11 | Support for model transforms         | PIM-PSM, PSM-PSM, etc. |
| P12 | Tracing                              |                        |
| P13 | Development lifecycle support        |                        |
| P14 | Use of standards                     |                        |
| P15 | Control and refinement of transforms |                        |
| P16 | Quality of generated code            |                        |
| P17 | Support tools                        |                        |

** OptimalJ: Three types of models
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.37369640787949016)
   :END:

- Domain model: high-level PIM (CIM?)
- Application model: PSM aspects such as J2EE implementation.
- Code model: generated from the application model.

** OptimalJ: Pattern types
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.46697566628041715)
   :END:


- Transformation patterns: patterns between models, transforming PIM
  to PSM, PSM to code, etc.
- Functional patterns: typical GoF patterns.

** OptimalJ: Protected regions
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.7560834298957126)
   :END:

Due to "open regions" and "protected regions", OJ is able to keep code
that was manually crafted between generations.

** ArcStyler: Cartridges
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.5214368482039398)
   :END:

According to the authors, this tool has a very advanced system for the
management of cartridges:

- they use UML profiles for the integration of cartridges with the
  modeling (for example, it defines a UML profile that exposes the EJB
  or Java 2 PSM functionality).
- there is a cartridge management system that handles them like
  plugins.
- its possible to define new cartridges from existing ones.

Info on the CARAT architecture can be found here:

- http://dis.um.es/~jmolina/documentos/CartuchosMDA.pdf

** Focus on design patterns
   :PROPERTIES:
   :NOTER_PAGE: (5 . 0.7300115874855156)
   :END:

Authors seem to imply that the extensive use of design patterns is
sufficient to determine the quality of the generated code. We disagree
with this opinion.

** Tradeoffs between integration and extensibility
   :PROPERTIES:
   :NOTER_PAGE: (7 . 0.38087520259319285)
   :END:

The two applications take very different approaches. OptimalJ focuses
its support mainly on J2EE and EJB. This allows it to provide
routripping and deep integration between PIM and PSM, and generate a
complete functional product. However, the company tightly controls the
product and users cannot extend it at will. On the other hand,
ArcStyle is openly extensible, but the downside is that cartridges
are responsible for making sure they interoperate. This means that
sometimes things don't work well with each other.

** Tracing
   :PROPERTIES:
   :NOTER_PAGE: (8 . 0.7293354943273905)
   :END:

It is very important to trace all elements such that one can tell what
model elements generated which files. Our tracing at present is mostly
concerned with obtaining model state before and after transforms and
is not designed for end users. However, a report detailing who
generated what would be extremely helpful.

** Bibliography

- MDA Guide Version 1.0.1. OMG. 2003.: More on model instance mapping
  and model type mapping in the
* An EMF-like UML generator for C++
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/An EMF-like UML Generator for C++.pdf
  :END:

Link: https://www.scitepress.org/Papers/2016/57448/57448.pdf

** objectives
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.379746835443038)
   :END:

- targets legacy systems that have been written in c++ and therefore
  can't easily make use of EMF.

** unification
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.5523590333716917)
   :END:

One of OMGs goals is to unify every step of the development process,
from requirements gathering to deployment. We should make it clear
which parts of the development process we are aiming to automate.

** Java is the only first-class citizen of EMF
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.785385500575374)
   :END:

EMF is great for those in Java but it does not aim to provide support
outside of this environment.

** Workflow
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.4142692750287687)
   :END:

From a Dogen perspective, UML4CPP is just another PDM (platform
definition model) and we could target it. It would allow us to
generate "MDE compliant" code, useful for certain scenarios. For this
we probably also need a UML4CPP/ecore specific facet which is
responsible for bridging the Dogen meta-model elements into
ecore. Actually this is probably not a good idea because to bridge the
gap between the two different representations we would end up creating
a lot of brittle glue code. We should just have two completely
different kernels - the regular MASD kernel and an EMF kernel.

This approach is useful in two ways:

- to create an EMF injector. For this we could use the eCore code to
  parse EMF models and then adapt these into dogen as we do with any
  other injector type. The generated code will look like regular dogen
  code, and it will make use of the MASD kernel. This is just using
  eCore as a bridge into Dogen.
- the second approach is to have a EMF/eCore kernel which generates
  code which has the full functionality of eCore including
  run-time/dynamic aspects. For this we rely on UML4CPP as a PDM.

** Annotations
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.7686996547756041)
   :END:

eCore models are augmented via annotations. We need to understand how
these differ from our variability approach and make sure our names are
in line with the EMF annotation terminology.

** Two-generator approach
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.5005753739930956)
   :END:

Instead of having a single generator for both eCore and user models,
there is one eCore generator and another just for user models.

** Reflection
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.5868814729574223)
   :END:

Use of reflection at run time is seen as an essential property of the
approach. In contrast in MASD we aim to avoid using reflection as much
as possible.

** UML OpaqueBehaviour
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.6214039125431531)
   :END:

Investigate this term.

** Memory management
   :PROPERTIES:
   :NOTER_PAGE: (5 . 0.21576524741081704)
   :END:

Because the UML4CPP framework knows how the model is structured, it
can generate efficient memory management code. This means using raw
pointers is possible in a safe manner. In Dogen we have a similar
knowledge of the user code. We could also take advantage of this by
introducing some kind of "model level pointer" class which results in
generating appropriate memory management code.

** Benchmark
   :PROPERTIES:
   :NOTER_PAGE: (6 . 0.35385500575373996)
   :END:

We could implement the test model in Dogen and perform similar
benchmarks to compare the performance of an EMF based kernel versus a
MASD based kernel in Dogen.

** Bibliography
   :PROPERTIES:
   :NOTER_PAGE: (8 . 0.3020713463751439)
   :END:

Interesting papers:

- Jungebloud, T., Jager, S., Maschotta, R., and Zimmermann,
  A. (2013). MOF Compliant Fundamentals for Multi-Domain System
  Modeling and Simulation. In Systems Conference (SysCon), 2013 IEEE
  International, pages 191–194. IEEE.

* An Abstraction for Reusable MDD Components
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/An Abstraction for Reusable MDD Components - Model-based Generation of Model-based Code Generators.pdf
  :END:

Link: https://dl.acm.org/doi/pdf/10.1145/1449913.1449940

Kulkarni, Vinay, and Sreedhar Reddy. "An abstraction for reusable MDD
components: model-based generation of model-based code generators."
Proceedings of the 7th international conference on Generative
programming and component engineering. 2008.


** Gist of the approach
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.4065998821449617)
   :END:

The paper describes a method for generation of code generators as a
hierarchical composition of reusable building blocks. A building
block is a localised specification; this is similar to our concept of
a physical space.

Authors propose a three step approach:

1. transform individual concern specific models into a unified model.
2. transform unified model into concern specific text artefacts
3. composition of the artefacts.

** Code patterns
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.760164997053624)
   :END:

Authors identify a separation of code into two types: business or
domain specific and architectural. For the architectural code, a
number of recurring patterns appear. These are effectively what we
have identified as SRAP.

** QVT default merging strategy
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.37124337065409546)
   :END:

In QVT the default "key based" merging strategy means that two
elements that have the same values for key properties are merged
together.

** Building block
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.4861520329994107)
   :END:

The unit of abstraction and composition they propose is the building
block. Presumably this is close to what we call a facet.

The composition proposed by this paper is akin to aspect oriented
weaving of blocks of text to form the final artefact. This is in
contrast to our approach which proposes such composition to be made
via facets. We should explain the pros and cons of both.

** Audit building block
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.5657041838538597)
   :END:

The paper proposes the creation of a building block responsible for
auditing. We could probably implement similar functionality in Dogen
using facets. Backlog this as a story.

#+begin_quote
This building block specifies how to maintain a persistent audit trail
of state changes of instances of a persistent class. Each persistent
class has a corresponding audit table having a column to store
time-stamp of the state change operation, a column to store the
pre-image and a column to store the post-image.
#+end_quote

** Issues with building blocks
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.601060695344726)
   :END:

These issues are similar to those we face in dogen.

1. The level at which we model building blocks is hard to gauge.
2. Its difficult to determine where one building block starts and
   another ends.
3. Separation of concerns of building blocks raises problems with
   tooling.
4. Tooling needs to support debugging BB.
5. Testing of building blocks should be possible in an independent
   manner.

** Bibliography
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.7690041249263405)
   :END:

- Harold Ossher, Peri L. Tarr: Hyper/J TM : Multi-Dimensional
  Separation of Concerns for Java TM . ICSE 2001: 821-822: HyperJ
  seems to have some similarities with stitchArchitecture-Centric
  Model-Driven Web Engineering.

* Architecture-Centric Model-Driven Web Engineering
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/Architecture-Centric Model-Driven Web Engineering.pdf
  :END:

Link: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.244.6866&rep=rep1&type=pdf

Escott, Eban, et al. "Architecture-centric model-driven web
engineering." 2011 18th Asia-Pacific Software Engineering
Conference. IEEE, 2011.

** AC-MDSD focuses on infrastructural code
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.3683241252302026)
   :END:

AC-MDSD captures structural patterns, mainly at the architectural
level. Due to this it does not focus on behaviour. Authors propose an
approach that allows architectural focus but with access to behaviour.

** Advantages of MDE for web apps
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.6721915285451198)
   :END:

- solves the boilerplate problem of web applications, that have a lot
  of commonality due to the tiered architecture.
- provides a model level understanding of architectural patterns,
  rather than having them scattered around code artefacts.
- provides re-usability because domain concepts are encapsulated in
  the model.

** Reference implementation driven approach
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.6906077348066298)
   :END:

The approach recommended by Stahl and Volter starts by creating a
reference implementation manually and then abstracting the general
(model-based) implementation from it. It has several advantages:

- high-quality code as a starting point, which is validated upfront
  using regular developer tools.
- the tooling produced is white-box, that is developers understand all
  of the inner workings of the tool unlike in vendor supplied tooling.

** White box and black box CASE tools
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.22099447513812157)
   :END:

- black box is where the tools hide the internals and provide a
  limited extensibility API. Users cannot modify or understand the
  tool at will.
- white box is where there is transparency of the inner workings of
  the tool Dogen is a white-box tool. It is important to provide a
  mapping between the model elements and the generated artefacts
  (e.g. tracing) at all levels of abstraction.

** Related work
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.5156537753222836)
   :END:

All of the approaches under analysis are dedicated solely to the
design of web applications, rather than attempting to generalise the
architectural patterns to enable applicability outside of this limited
scope. This is part of the issue Dogen tries to address.

** Graphical vs Textual
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.6629834254143647)
   :END:

Graphical notation is good for expressing intent visually whereas
textual notation is good for "structured summary presentation".

** DDD Entity pattern and manager (service)
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.5432780847145489)
   :END:

We need to review Evans (DDD) on entity pattern and manager. If what
this paper says is correct, this would be very amenable to code
generation because we separate behaviour (manager) from structure
(entity).

** Two-track development approach
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.7918968692449356)
   :END:

We should write a critique of this approach, and why we have chosen to
distil its principles but not partake in the exact approach in Dogen.

** AC-MDSD approach with regards to extensibility
   :PROPERTIES:
   :NOTER_PAGE: (7 . 0.423572744014733)
   :END:

The capturing of new patterns for Dogen is expected to be done
organically. This we share in common with AC-MDSD as described in
this paper.

** Bibliography
   :PROPERTIES:
   :NOTER_PAGE: (8 . 0.3775322283609577)
   :END:

- find multi-stage transformation process p188 in Sthal. The approach
  in dogen is to flatten the multi-stage pipeline into an
  heterogeneous meta-model. We should describe in detail the
  differences between the two approaches.
- we are using model-based testing in Dogen, so we need to find
  literature for this. E. Escott, P. Strooper, J. Steel, and P. King,
  “Integrating Model-Based Testing in Model-Driven Web Engineering,”
  in Proceedings of the Eighteenth Asia-Pacific Software Engineering
  Conference, 2011

* A UML Profile for Feature Diagrams: Initiating a Model Driven Engineering Approach for Software Product Lines
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/A UML Profile for Feature Diagrams: Initiating a Model Driven Engineering Approach for Software Product Lines.pdf
  :END:

- Possompès, Thibaut, et al. "A UML Proﬁle for Feature Diagrams:
  Initiating a Model Driven Engineering Approach for Software Product
  Lines." Journée Lignes de Produits. 2010.
- Link: https://hal-lirmm.ccsd.cnrs.fr/lirmm-00542800/document

** Objective is full lifecycle
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.29374605180037905)
   :END:

The authors of the paper are targeting the analysis and requirements
gathering as well as the more traditional development modeling. We
have no such requirements. However they do target SPL as we do.

** Feature model elements
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.786481364497789)
   :END:

Key elements of feature model:

- product line contains features.
- product contains features and belongs to a product line.
- features associated with a product may have constraints such as
  require relations and mutual exclusions.
- features have feature properties. These describe feature parameters,
  or properties chosen by the user.
- variability kinds:
  - fixed: constant
  - variable: van be changed in a product, depending on other features.
  - family variable: can change from product to product.
  - user defined: given as input by the user.
- feature can have sub-features. This is a better term than feature
  bundles.

** Sub-features
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.6727732154137713)
   :END:

As with most papers on the subject of feature models, the grouping of
features is done mainly to allow for complex relationships between the
features in the group. Our main use case is quite trivial, we just
need to group features as if in a "package". We could of course make
use of a relationship such as the =AndGroup= but its not clear why one
would implement all of the complex logic around feature relationships
if we only require the simplest use case.

However, the authors put forward the notion of a "feature set", which
seems to be the ideal candidate for our "feature bundles". Feature
sets group features from an arbitrary point of view. This is what we
do when modeling features.

Feature sets enable complex relationships between features and feature
sets themselves. We could probably use some of this - for example, if
ORM stereotypes were features, we could determine which ORM features
are enabled or disabled on the back of it. However, this would require
a generalisation of a lot of the handling that has been hard coded
thus far and its not obvious what advantages would be gained by this
generalisation.

** UML Component as a feature
   :PROPERTIES:
   :NOTER_PAGE: (6 . 0.7770056854074542)
   :END:

Authors use components to model features because ports allow
expressing groups of associations of sub-features. Since we do not
have a need for complex groups, the simple grouping that attributes
enable us to do is sufficient.

** Model-relationship
   :PROPERTIES:
   :NOTER_PAGE: (7 . 0.2368919772583702)
   :END:

Investigate what a model relationship is. Presumably it is the
meta-type for aggregation, etc.

** Bibliography
   :PROPERTIES:
   :NOTER_PAGE: (12 . 0.6064434617814277)
   :END:

- [ASI 06] A SIKAINEN T., M ANNISTO T., S OININEN T., “A unified
  conceptual foundation for feature modelling”, SPLC ’06: Proceedings
  of the 10th International on Software Product Line Conference, IEEE
  Computer Society, 2006, p. 31–40.
- Check that this paper is the same one as we read in English: [CLA
  01] C LAUSS M., Untersuchung der Modellierung von Variabilität in
  UML, Technische Universität Dresden, Diplomarbeit, 2001.
- [POS ] P OSSOMPÈS T., D ONY C., H UCHARD M., R EY H., T IBERMACINE
  C., V ASQUES X., “Design of a UML profile for feature diagrams and
  its tooling implementation”, submitted. Gathering of requirements
  for the feature model.
* Generic Modeling using UML extensions for variability
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/Generic Modeling using UML extensions for variability.pdf
  :END:

- Clauß, Matthias. "Generic modeling using UML extensions for
  variability." Workshop on Domain Specific Visual Languages at
  OOPSLA. Vol. 2001. 2001.
- Link: http://dsmforum.org/events/DSVL01/clauss.pdf

** Feature models target end users
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.8137583892617449)
   :END:

Authors agree with the general view that feature models are most
useful to model end-user related properties of the system. With Dogen
the line is blurred because our end-users are regular developers.

** Instantiation of models into products
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.16778523489932887)
   :END:

The authors take the view that the product line contains generic
models which are instantiated into concrete products and parameterised
via variability. Our view is similar, but we should probably make it
clear that our models are not very generic; only the high-level
structural patterns for a product line are meant to be captured and
its not really accurate to call dogen's models "generic".

** Variation points
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.2158273381294964)
   :END:

Variation points provide a useful way of handling variability.

** Generic models from Domain Engineering
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.5312672938572219)
   :END:

The objective of a generic model is to describe a product line
architecture; it contains a model of the variability and this model
must be bound - e.g. instantiated - in order to generate concrete
software artefacts.

** Hiding of less important information
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.19092418372993913)
   :END:

The author explains that tagged values should probably be hidden by
the tool given that they are not important for many use cases in
modeling. We will also have a similar approach when we move to the new
format for input.

We should discuss perceivability in the context of the new injector.

** Variation points
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.3154399557277255)
   :END:

The variation point allows having more than one "implementation" for a
given feature, and binding it to a user choice in the user model. In
this sense, we can say that our approach also entails variation
points, but these are encoded in the logical-physical space. We need
to compare and contrast the two approaches.

** Model evolution
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.7470946319867183)
   :END:

We should explicitly state this as a non-goal for the present version
of Dogen.

** Variation points and JSON
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.3901494189263973)
   :END:

One case where we need something extremely similar to variation points
is in allowing multiple implementations of a given "feature". For
example JSON serialisation. There are many libraries in C++ that
satisfy this requirement and each user may use one library for their
own reasons. However, it would be nice to be able to state that the
JSON feature is enabled without having to concern ourselves with the
specific implementation chosen. This seems to be very close to VPs.


** Binding times
   :PROPERTIES:
   :NOTER_PAGE: (5 . 0.5561704482567792)
   :END:

Authors provide a very fine grained approach to binding times. In
Dogen we took the simplest possible approach and all binding is done
at generation time. Presumably =build= time in the paper's categories.

** Variability at the model element level
   :PROPERTIES:
   :NOTER_PAGE: (5 . 0.7304925290536801)
   :END:

We should make it clear that we do not allow "too much" (to be defined
precisely what is meant by this later) variability in the logical
dimension. In other words, you cannot use parts of a object definition
based on variability parameterisation. There is the exception of
object templates.

** Bibliography
   :PROPERTIES:
   :NOTER_PAGE: (8 . 0.6889872717210846)
   :END:

- M. Clauß, Modeling variability with UML, GCSE 2001 - Young
  Researchers Workshop, September 2001
- M. Clauß, Untersuchung der Modellierung von Variabilität in UML,
  diploma thesis, August 2001. Try to locate an English translation.

* Using Aspects to Model Product Line Variability
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/Using Aspects to Model Product Line Variability.pdf
  :END:

- Groher, Iris, and Markus Voelter. "Using Aspects to Model Product Line
  Variability." SPLC (2). 2008.
- https://pdfs.semanticscholar.org/4c77/0315cd8151f6c162ac2f99ecc62225f4c94e.pdf?_ga=2.246561604.1739388568.1592151663-6190553.1592151663

** Product line engineering
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.636140135218193)
   :END:

- SPLE takes advantage of the commonalities between products in a
  family to improve reuse.
- products in a family differ from the features that have.
- features are increments in functionality.

** MDSD and AOSD
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.8297480024585125)
   :END:

The authors put forward a merge between these two approaches as a way
to manage the entire lifecycle of variability, as well as dealing with
the cross cutting nature of most features. They also argue that moving
to a model level abstraction permits a more compact and therefore more
manageable view of variability.

** Domain Engineering / Application Engineering
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.17516902274124155)
   :END:

Dogen uses the exact same approach as DE/AE, except the DE portion of
the work is related to the development of Dogen itself and the AE
portion of the work is the application of Dogen to user projects, done
via configuration.

** Model-level weaving
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.4425322679778734)
   :END:

The approach put forward results in the weaving of model elements,
something we are avoiding by design as it increases the complexity of
the modeling process quite a lot.

We need to compare and contrast the weaving that is performed by
object templates and configurations against the weaving of model
elements proposed by the paper. We should use weaving terminology for
these terms. We also should mention that after weaving you cannot tell
that weaving took place.

** Aspects for variability increases flexibility
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.6822372464658881)
   :END:

By using aspects, the model does not need to be prepared for
variability in advance. We take the contrasting view: we only support
very limited cases of variability, and these must be exposed
explicitly. Our approach is by design less flexible.

** Comparison between our approach and aspects
   :PROPERTIES:
   :NOTER_PAGE: (3 . 0.8205285802089736)
   :END:

The multidimensional approach put forward in Dogen overlaps to an
extent the AOM approach for variability in the paper. However we focus
on coarse grained features, we don't provide flexibility and
composition is only allowed in very narrow circumstances. We need to
provide a comparison of both approaches. However, we also have a clear
separation of the model elements and the variability modeling.

* A flexible code generator for MOF-based modeling languages
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/A flexible code generator for MOF-based modeling languages.pdf
  :END:

- Bichler, Lutz. "A flexible code generator for MOF-based modeling
  languages." 2nd OOPSLA Workshop on Generative Techniques in the
  context of Model Driven Architecture. 2003.
- Link: https://s23m.com/oopsla2003/bichler.pdf

** MOmoC
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.5128923766816144)
   :END:

Authors put forward an approach that uses XMI as input, and parses it
into XML and then uses XSLT and stylesheets to convert the XML into
source code. From experience, we know this is not a scalable approach
as its too low-level.

** Model compiler
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.30269058295964124)
   :END:

Authors reference model compilers, and reference OMG but do not
provide a definition of a model compiler.

** MOmoC architecture
   :PROPERTIES:
   :NOTER_PAGE: (2 . 0.45403587443946186)
   :END:

- frontend is generated code that reads MOF models in XMI. Supports
  targetting the generation to other representations that are
  MOF-compliant.
- middle-end is supplement by user defined modules. At present it has
  a type mapping and naming resolution modules. Middle-end transforms
  types into XML
- backend is made up of XSLT that generate the implementation code.


Review of the paper: Groher, Iris, and Markus Voelter. "Using Aspects
to Model Product Line Variability." SPLC (2). 2008.
* A Comparison of Generative Approaches: XVCL and GenVoca
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/A Comparison of Generative Approaches: XVCL and GenVoca.pdf
  :END:

- Paper: Blair, James, and Don Batory. "A Comparison of Generative
  Approaches: XVCL and GenVoca." Technical report, The University of
  Texas at Austin, Department of Computer Sciences (2004).
- Link: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.457.1399&rep=rep1&type=pdf

** Compositional Design
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.19493177387914232)
   :END:

The idea is to start with a core and add progressively to it, with
each addition roughly corresponding to a feature. We propose a
compositional approach but based on multi-dimensionality.

** References

- Programming by difference: R.E. Johnson and B. Foote, “Designing
  Reusable Classes”, Journal of Object-Oriented Programming,
  June/July 1988.
* An evaluation of the Graphical Modeling Framework GMF based on the development of the CORAS tool
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/An evaluation of the Graphical Modeling Framework GMF based on the development of the CORAS tool.pdf
  :END:

- Paper: Seehusen, Fredrik, and Ketil Stølen. "An evaluation of the
  graphical modeling framework (gmf) based on the development of the
  coras tool." International Conference on Theory and Practice of
  Model Transformations. Springer, Berlin, Heidelberg, 2011.
- Link: http://hjem.ifi.uio.no/ketils/kst/Articles/2011.ICMT.pdf
** Tagging in GMF
   :PROPERTIES:
   :NOTER_PAGE: (4 . 0.4036697247706422)
   :END:

Investigate how GMF code that is modified after code generation is
tagged so that the generator knows not to overwrite it.

Actually XPand allows for annotations in methods of the generated code
that indicate whether the methods should be overwritten or not.

** Packaging issues
   :PROPERTIES:
   :NOTER_PAGE: (10 . 0.6324786324786325)
   :END:


Authors argue that for a Tool B that reuses Tool A, it is not possible
to merely reference a package that contains both the source code of A
and the models of A; instead they ended up copying and pasting A into
B. It is not clear why the packaging of model plus source does not
work.
** Avoid human interactions in generations
   :PROPERTIES:
   :NOTER_PAGE: (12 . 0.2222222222222222)
   :END:

Authors argue that the generation process should be as mechanical as
possible, and should not have any human interacts, particularly using
UIs.

* Features as transformations: A generative approach to software development
  :PROPERTIES:
  :NOTER_DOCUMENT: papers/Features as Transformations: A Generative Approach to Software Development.pdf
  :END:

- Paper: Vranić, Valentino, and Roman Táborský. "Features as
  transformations: A generative approach to software development."
  Computer Science and Information Systems 13.3 (2016): 759-778.
- Link: https://pdfs.semanticscholar.org/7f20/ee0ef94ba20161611c2ae184e6040f9d2fe1.pdf?_ga=2.47007141.386256099.1594564659-1149343892.1591869910

** Feature selection
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.7960644007155635)
   :END:

Authors claim that feature selection is a much more complex process
than merely enabling a component. We are striving to make it so.

** Feature interaction
   :PROPERTIES:
   :NOTER_PAGE: (1 . 0.8318425760286225)
   :END:

The fact that features interfere with each other and may require
adaptation in order to be applied to the system.

** Classification of transforms
   :PROPERTIES:
   :NOTER_PAGE: (7 . 0.5277280858676208)
   :END:

Authors provide a simple classification of transforms which can be
used also to describe the SRAPs in Dogen.

* Translating Alloy Specifications to UML Class Diagrams Annotated with OCL

- https://repositorium.sdum.uminho.pt/bitstream/1822/35641/1/848.pdf

* Agile MDA

- "An executable model, because it is executable, can be constructed,
  run, tested and modified in short incremental, iterative cycles."
  Executability is not a requirement for agility. Dogen models provide
  exactly this type of development.
- they fall into the voltaire pitfall: "To eliminate the verification
  gap and enable immediate delivery of fragments of the system, what
  we need is a highly abstract modeling language that focuses on a
  single subject matter—the subject matter of interest to the
  customer—and yet is specific and concrete enough to be executed, an
  executable model."

- Model Compilers: "A model compiler takes a set of executable UML
  models and weaves them together according to a consistent set of
  rules. This task involves executing the mapping functions between
  the various source and target models to produce a single
  all-encompassing metamodel (aka “the grungiest”) that includes all
  the structure, behavior and logic—everything—in the system. The
  final mapping from this metamodel can be done in several ways. One
  approach to defining mapping functions is to use an archetype, which
  are especially suited to manipulating text. Weaving the models
  together at once addresses the problem of architectural mismatch, a
  term coined by David Garlan to refer to components that do not fit
  together without the addition of tubes and tubes of glue code, the
  very problem MDA is intended to avoid! A model compiler imposes a
  single architectural structure on the system as a whole."

* Model-driven Development of Complex Software: A Research Roadmap

- [[file:papers/Model-driven%20Development%20of%20Complex%20Software:%20A%20Research%20Roadmap.pdf][paper]]
- MDA talks about CIM, PSM, PIM. In dogen we advocate a mix of PIM and
  PSM. Users should try as much as possible to keep their models PIM,
  but if need be, just drop to PSM. CIM and PIM are really a concept
  that is meaningful at a level higher than dogen.
- we do not think that automated platform migrations are a realistic
  target so we do not support this use case.
- we do not believe in the industrialisation of software
  development. The activity is too complex to be automated. However,
  this does not mean that a small amount of automation is not
  possible. Voltaire: le mieux est l'ennemi du bien.
- "Reusable forms of development experience (e.g., patterns,
  templates, guidelines, transformations) are associated with each
  viewpoint, and thus accessible in the context of that viewpoint." We
  are trying to do this with facets.
- separation of design concerns: facets can be understood as different
  design concerns. maybe there is a need to talk about macro-concerns
  (architectural) and micro-concerns (class-level). Facets can span
  both but we are only worried about micro for now. However, when it
  comes to describe the generation model we will have to address this
  issue.
- it seems facets are an AOM (aspect oriented modeling)
  approach. However, instead of allowing multiple viewpoints and
  composition, we treat aspects/facets as variation points and the
  composition is pushed down to the (feature) configuration
  level. This solves a lot of the difficulties around composition of
  primary and aspect models.
- facets focus only on trivial behaviours, making composition
  trivial. Also, in general, they are implemented external to the
  class so that merging is not an issue and its much easier to isolate
  conflicts. However, this must be done manually by the designer of
  the class, and stipulated as a rule (e.g. facet a depends on b and
  conflicts with c).

* A meta-model for language-independent refactoring

- [[file:papers/A%20Meta-model%20for%20Language-Independent%20Refactoring.pdf][paper]]
- Why they created their own metamodel: "Another reason is that models
  such as UML [18] are directed towards object-oriented analysis and
  design rather than source code representation." This also applies to
  dogen.
- the pre and post conditions may have some use for dogen
  validation. For example adding new classes, methods etc.
- we could also reuse the analysis on the costs:
  - "Increased complexity of algorithms. To deal with multiple
    languages the underlying model needs to be general enough to cover
    the supported languages."
  - "Mapping back to the actual code. The actual code changes are,
    naturally, language specific. However, in some cases the concepts
    that are generalized at the language-independent level (e.g. Java
    constructors are methods, Java interfaces are classes) need to be
    mapped back to their language-specific kind, because at the code
    level they need to be dealt with differently than their ‘normal’
    counterparts."
  - "Language-independent defaults. To keep some refactorings as
    language independent as possible, some defaults are used."
- similarly, section "Not all language differences can be abstracted
  from." also has a lot of good points.

* Metrics on Feature Models to Optimise Configuration Adaptation at Runtime

- paper not in our library
- a use case for this paper is facet enablement, In an ideal world, we
  should be able to look at the included files in a project and
  backout the used facets. We can then determine what is not being
  used (diff generated files from included files). Then we can propose
  a list of facets to disable (or types to remove) to the user. This
  also solves the TB problem: delete types that are not in use. It has
  to be robust enough to understand graphs of types (e.g. no includes
  of base, but derived etc). Interestingly, we only need the includes
  (no deep parsing of the AST). However, we do need to know of all
  consumers of a model. Perhaps we could make use of public/internal
  separation: types marked as public are excluded. The second problem
  is that we cannot easily patch the model with the resulting facet
  enablement.
- however the approach given on this paper does not apply directly to
  our needs because we never need to do a search in a space. Our use
  cases are:
  - 1. user selects a feature configuration, we validate it. If
    invalid, throw and tell the user why. User has to manually fix it.
  - 2. user selects a feature configuration and we automatically
    enable/disable any features based on dependencies. If we can't
    enable a feature (not supported), throw and tell the user (or
    possibly disable the graph using the feature).
  - 3. user points to a code base and we determine what files have
    been included. From this we back out used features. We diff this
    against the list of generated features and automatically propose a
    feature minimal configuration.

  None of these cases requires searching through a feature space.

* A UML profile for feature diagrams

- [[file:papers/A%20UML%20profile%20for%20feature%20diagrams:%20Initiating%20a%20model%20driven%20engineering%20approach%20for%20software%20product%20lines.pdf][paper]]
- this meta-model is very useful for dogen. We could implement it as a
  model and then extend it with the requirements needed for dogen (is
  it a global feature or element feature, more fine-grained types,
  etc). The model should expose annotations as a flat representation
  of the rich feature model. It should convert from annotations into
  the model, performing validation. It should also validate
  dependencies (build a graph of features).
- in addition, we need to create a UML profile to expose the model,
  and then create a dogen model with the profile. The generated code
  should read annotations and return instantiated features with the
  correct types.
- we should try to make this model generic so that it can be used by
  applications. However, given that there is no notion of annotations,
  we probably need a different way of expressing it. Perhaps we could
  use boost property tree. In fact that is the right approach: boost
  property tree becomes the underlying representation; we make
  =Annotatable= contain a property tree; the generated code generates
  the C++ data types that represent the configuration, and it has a
  load/save (not quite those names) to serialise and deserialise
  itself from a property tree. When defining a feature we need to
  supply its "path", e.g. the path to the class. The path to the
  attribute is inferred by the name. Now this code is completely
  agnostic to code generation - it is merely a way to do
  configuration. We just need to add the notion of scopes so that we
  can distinguish between element and model level options. Actually
  the notions of global and local are not dependent on code
  generation, but what global and local mean are.
- for extra bonus points, it would be nice to be able to map features
  to program options as well; instead of property tree, we would
  generate the code to setup the program options as well as the class
  to represent the options. The user could specify the implementation
  mapping as meta-data (a feature).
- note that we can just map a feature set to a class because then its
  not possible to define the complex relationships between features
  (and/or etc). These are required particularly if we want a program
  options mapping. The approach in the paper is not ideal but at least
  it solves this problem.

* AndroMDA

- [[file:papers/AndroMDA%20-%20German.pdf][paper]] (german)
- MDA: "The transformation of the models into platform-specific source
  code requires tools and frameworks."
- AndroMDA generates ready to use applications; we focus on extracting
  simple structural functions.
- AndroMDA is designed to fit into MDA, we are targeting unorthodox
  practitioners.

* Construction and Evolution of Code Generators

- [[file:papers/Construction%20and%20Evolution%20of%20Code%20Generators:%20A%20Model-Driven%20and%20Service-Oriented%20Approach%20-%202013.pdf][thesis]]
- "In essence, even though there are code generator workbenches that
  help the programming expert to design dedicated code generators,
  more high-level activities like process support, variability
  management, or product lining are not foreseen."
- jorges code generator has support for constraint solving and model
  checking.
- jorges points to AndroMDA, seems very similar to what we are trying
  to do. We need to find more about it and explain why we are
  different.
- one of the problems of this approach is that we now have to deal
  with SIBs and jABC: a completely different paradigm from how
  developers are used to working. What we really need is an
  abstraction that is very close to the code first.
- the library of common SIBs is similar in spirit to a library of
  common facets.
- our target is only the developer. There are no features for domain
  experts, business analysts, etc.
- we do not want XMDD given it aims for full generation. We need
  something more like Agile MDD with barely good enough models and
  modeling.
- we need to look at their constraints checker and see if any of the
  constraints or the approach can be lifted for dogen.
- in order to do model checking one needs to be familiar with modal mu
  calculus. Most regular developers won't so very few will add checks.
- facets are part of our strategy of variant management. The variation
  points are metadata parameters that allow configuration of the
  facet.
- we have a similar notion of execution context, but ours is strongly
  typed. This may not be flexible enough for its requirements.
- the context could also contain the error stack with all errors found
  so far (validation etc).
- unlike genesys, in dogen we have a fixed interface for facets and
  they are always consumed via the metadata. We don't expect users to
  create new code generators, we only allow variability via
  metadata. Two use cases: 1) code generator developer, either does a
  new kernel (very infrequent) or adds a new facet to the facet
  library 2) code generator user: configures the variability.
- benchmarking is probably helpful: figure out where in dogen time is
  going. We already have probing for this but it does not give details
  on individual facets.
- meta-information about facets would be helpful. It is useful for
  example for listing the available facets (with a description), their
  configuration parameters, their dependencies. Similarly with kernels
  and backends.
- facets and aspects are implementations of features.
- we should have exactly a three-phase approach in dogen: 1)
  initialisation 2) transformation 3) generation. We already have this
  except we mapped the generation phase as a transform. However, in
  the thesis we can describe the three phases like Jorges (p119). This
  also means that all of the "static" registration we have at present
  can now be placed into the initialisation phase. If we had a
  top-level "phase manager" we could simply call the phases from
  main. We must also keep in mind the "service" approach, where we may
  process more than one model (e.g. transformation + generation are
  applied repeatedly).
- however, the three phase approach has to do with orchestration (it
  will manifest itself as classes or methods in orchestrator). The
  code structure should follow a frontend, middleend, backend
  pipeline. This is more suitable than external, modeling and
  generator.
- we need to also have an ecore example/case study where we should
  make the same points as ch7: dogen does something that no other EMF
  tool does (facets).

* Bridging the Gap Between Features and Models

- [[file:papers/Bridging%20the%20Gap%20Between%20Features%20and%20Models.pdf][paper]]
- the variant model is in effect a feature configuration. A
  realisation of the feature model. The variant model is used in the
  solution space to determine what features to include.
- there are two types of features: "We differentiate two types of
  features: features that have realisations connected to specific
  points in the core and features that add duplicated artefacts
  scattered across various points in an aspectual manner."
- "In our work we differentiate two feature types: collaborative
  features and aspectual features." This separation is very useful for
  dogen features: we need to classify dogen features, and explain how
  they map implementation-wise. However, we need to also state that we
  do not offer a generic mechanism to the user - instead, the user
  should use whatever technologies are available for modeling.
- in order to meet our requirements, dogen does not rely on any
  existing MDE technology; it is designed to integrate with existing
  tooling but does not require it. Thus we do not make use of ecore
  etc.

* Generating Aspect Code from UML Models

- [[file:papers/Generating%20Aspect%20Code%20from%20UML%20Models.pdf][paper]]
- "Code generation improves developer productivity, ensures
  syntactical correctness and reduces errors when mapping model to
  code."

* State of the art of QVT: a model transformation language standard

- [[file:papers/LNCS%20-%20Applications%20of%20Graph%20Transformations%20with%20Industrial%20Relevance%20-%202007.pdf][paper]]
- explains the motivation behind PSM/PIM: "This classification is
  motivated by the constant change in implementation technologies and
  the recurring need to port software from one technology to another."
- the ideas behind QVT are interesting, but in some ways the DSL makes
  it harder to understand them. If instead we simply had a library
  with MOF and then say two metamodels (MOF instances): relational
  model and UML model. Then we could define a function that
  transformed one metamodel element to another, say =class= to
  =table=. So in terms of programming, one could imagine a reflection
  based API with "objects" where each object could have a metatype
  (recursively, until reflexivity takes over):

#+begin_example
class o;
o.name("my_class");
...
if (o.metatype() == class) {
    table t;
    t.name(o.name());
}
#+end_example

- this is most useful if the code generation model is able to generate
  arbitrary code - e.g. the structural patterns which we are encoding
  into the code generation metamodel are now expressed as
  transforms. This allows for arbitrary structural patterns, but it
  does mean: a) the code generation metamodel must be very generic b)
  the transforms are much more complex.
- in the case of a financial model, we would create a DSL with all of
  the financial products. Then, we'd instantiate the DSL with the
  actual products supported by the company. So a vanilla option for
  example is implemented as a deliverable product, with cashflows
  etc. Then we transform this model into several other metamodels
  (code generation metamodel, UI metamodel, etc). For example the UI
  metamodel could have concepts such as form, button etc. To represent
  a vanilla, we would transform the financial product metamodel into
  the UI metamodel:

#+begin_example
vanilla v;
form f;
f.title(v.name());
...
#+end_example

  Each of the individual elements of the products will have mappings
  to the UI metamodel. Then, the UI metamodel is code generated by a
  kernel specialised on the UI metamodel. Metamodels and generators
  come in pairs. These pairs are kernels: OO, relational, XML/XSD, UI,
  etc. Features can be implemented either as facets/aspects on an
  existing kernel (e.g. ODB for relational) or, if they have their own
  metamodel, as a kernel (e.g. relational kernel). Kernels can depend
  on other kernels (e.g. the relational kernel depends on the OO
  kernel because it creates c++ code that reads/writes c++ objects to
  a relational database). Users create their metamodels and models in
  ecore, which we then transform to the kernel models.

* Generative Programming Using Frame Technology

- [[file:papers/Generative%20Programming%20Using%20Frame%20Technology.pdf][paper]]
- in general it seems there isn't much of frame technology we can
  reuse. However there are a few concepts: frame layers, framework and
  frame libraries. However, where frames are very configurable and
  reusable, facets are designed to have as few variation points as
  possible and to be reused as is. You either need boost serialisation
  or you do not - there isn't a lot of scope for variation points.
- dogen is a single-phased generator: "A single-phased generator (see
  Figure 3.12:2) works sequentially. It takes its input (see Figure
  3.12:1) which consists of specific parameters and commands mixed
  with code of the target language and processes it. In case of a
  frame processor these are the frames and functions. During the
  evaluation of the input source, the generator can use data from a
  repository (see Figure 3.12:3) to enhance or check the input. A
  repository in this context means any kind of data storage that
  contains configuration data (e.g. a database or a
  XML-file). Finally, it produces an output (see Figure 3.12:5) with
  the production process running mostly in the same order as the input
  is structured. These outputs are often volatile. Every piece of
  generated source code is instantaneously released and can not be
  further processed, or evaluated for the processing of other source
  code [DST02b, p. 15]."
- the concept of frames and frame instances is useful: maybe we can
  use it when defining the facet space and the implementation
  space. But really artefacts are archetype instances already.
- frame processors use command line arguments. We are against this
  approach. We should probably explain why variability should be
  contained within the model and not leaked through to the command
  line.

* Ecore.Fmp: A Tool For Editing And Instantiating Class Models As Feature Models

- [[file:papers/Ecore.Fmp:%20A%20Tool%20For%20Editing%20And%20Instantiating%20Class%20Models%20As%20Feature%20Models.pdf][paper]]
- typical usage of EMF "Ecore is a part of the Eclipse Modeling
  Framework (EMF), a framework that provides a practical foundation
  for building modeling tools. When using EMF, users typically first
  create a metamodel for their domain in the form of an Ecore class
  model. Next, the users may use a code generator that generates an
  implementation of the metamodel or they can utilize the metamodel
  using reflection. The code generator Figure 2 shows a feature model
  that represents the class can also create a specialized graphical
  tree editor for ing and editing object models for the given
  metamodel."
- ecore annotations are similar to ours: "Ecore, similarly to
  Essential MOF, also supports a tagging mechanism which allows
  annotating each class model element with a set of key-value
  pairs. In Ecore, instances of the class EModelElement can contain
  many instances of the class EAnnotation , which, in turn, has a
  String to String map that represents key to value pairs. The
  complete metamodel of Ecore can be found at [1]."
- perhaps what we are trying to do is to model the feature model as a
  fixed set of classes - in effect, the feature model metamodel; then
  add the logic around it (graph, dependencies and so forth) as
  manually written code; then allow dogen developers to define a
  concrete feature model (an instance) by making use of a UML profile,
  which then generates code that instantiates the feature model
  instance; then when we are processing a user UML model, the user
  supplies a feature configuration which is an instance of the dogen
  feature model instance. Its all done at run time. We do not need to
  do this at run time for all of the features that make up the core of
  Dogen - we could have properly typed c++ classes. Then there should
  be a type layer which is part of the code generated feature model
  instance which can be instantiated from the feature model
  metamodel. We need to use the ecore.fmp mappings to convert say
  feature groups into classes and features into properties. The
  generated code automatically knows how to try to bootstrap one of
  these classes from features. In effect we have three layers:
  - annotation: KVP layer
  - feature model: dynamic layer with feature model metamodel.
  - feature model instance: set of c++ classes with a typed
    representation of the actual feature model. Inject typing into the
    feature model.
  - feature configuration: user UML diagram making use of the
    available features.
- interestingly, feature models do not have any binding in terms of
  scope to metamodel elements. Thus the FM metamodel will not have
  this concept. However, for dogen we need this. We could introduce
  the notion of local and global - in practice this is sufficient for
  all of the existing use cases. Or we may just have to hack the
  feature metamodel specifically for dogen.
- which perhaps raises the point of: are features not just modeling
  elements, present in the dogen codegeneration metamodel? This would
  make life easier and add more research weight. If we did fold the
  feature metamodel into the metamodel this would have the following
  consequences:
  - we would need a transformation that creates value objects from
    features and converts them into the c++ typesystem.
  - we need a mapping that reads these typed objects in and out of
    feature metamodel elements.
  - we need some manually crafted code that converts an annotation
    into instances of these feature metamodel elements.
  - we need manually crafted code that handles global/local,
    overrides, etc.
  - we need a template that injects instances of the feature metamodel
    elements.
  - we need some way of registering the c++ types so that we can
    generate them from the feature metamodel. Or perhaps this is done
    on the fly as users ask for them.
  - we must not bind feature metamodel directly to annotations; it
    should work against any KVP representation with the concepts of
    local and global.
  - one slight snag though: in order to use the feature metamodel in
    their own code, users now need to link against the dogen
    metamodel. So this is a dogen specific thing. If however, we were
    to construct the needle model and make it independent of dogen, we
    could move it there. It would make more sense to have a feature
    model as an external library like [[https://github.com/EmilianoSanchez/Feature-Model-Optimization][Feature Model Optimization]]. Or
    maybe there are two aspects to it: first there is the external
    library that does all the computations and optimisations to FM and
    provides a non-typed API to features. Then there is the dogen
    support, which does several things: 1) maps features to codegen
    metamodel; 2) instantiates the external library based on the
    metamodel 3) creates strongly typed representations of
    features; 4) maps external library to strongly typed.
- future work: how to plug user defined feature models back into dogen
  such that users can make structural features (classes, attributes)
  dependent on feature configuration. We do not have a use case for
  this. at present there are two separate worlds: 1) general feature
  model 2) dogen's usage of general feature model for internal
  purposes 3) dogen's generation of helper code on top of general
  feature model so that users can make use of it. This would be a
  fourth use case.
- the paper seems to allude to a solution on the cyclical references
  in feature configuration, though its not obvious how it works: "In
  the case of feature configuration, the upward branch traversal is
  stopped when an instance is reached. If none of the elements in the
  cyclic containment are a root feature, then the user is required to
  annotate the desired EClass that should be the root feature using
  the annotation root as discussed earlier."

* Towards Separation of Concerns in Model Transformation Workflows

- [[file:papers/Towards%20Separation%20of%20Concerns%20in%20Model%20Transformation%20Workflows.pdf][paper]]
- definition: "modeling workflow (workflow): It emphasizes that there
  may be other tasks to specify than only model transformations, for
  example model loading, storing, checking, or code generation."
- shortcomings of workflow technologies: 1) integration effort: hard
  to add further transformation technologies 2) flexibility: hard to
  specify additional processes 3) inadequate variability management
  support.
- this paper provides a basic analytical framework with which to
  evaluate modeling workflows. We could apply it to Dogen and explain
  why we have taken the present approach. Its probably beyond the
  scope of the thesis, but suitable for the Dogen manual.
- we can also benefit from making workflows more explicit with named
  components. This could be the start of the natural evolution towards
  a DSL. Types of workflow elements: model checkers, creators,
  transformers, finishers, chains.
- they defend the integration of multiple technologies, whereas we are
  against it. It raises the complexity bar even further. We believe
  there should be interfaces defined at a higher level such as using
  ecore as an input model to the generator. We have no bearing in what
  is done prior to the generation of the ecore model. The code
  generation workflow is completely decoupled from any preceding
  workflow that was used to generate the ecore model.
- using a DSML for workflow generation is similar to using workflow
  engines for other purposes: you end up encoding all of the FSM logic
  in fragments of XML that are extremely hard to debug and manage. You
  loose all of the affordances that regular GPL code gives you. It may
  appear to be more expressive but the end result is that you spend
  more time trying to get things to work.
- this hits the nail on the head with our notions of fractal
  engineering: "The strict separation of concerns, which we foster
  throughout this paper, has a particular reason: we especially are
  interested in the terms and conditions for the decomposition and
  composition of whole product lines. Product generation of a compound
  product originated from several product lines is a potentially
  highly complex task. Several product generation processes, one of
  each product line, may have to interact to create the final
  product. Thus, a clear interface for interaction is necessary, and,
  for model-driven product lines, model transformation workflows
  result to be a promising integration point for that purpose."
- however, our approach is to decouple product interfaces. A product
  line may make use of another product's interface but we should never
  need to assemble one product from several products. The unit of
  development is the product, and product lines make products. A
  product may rely on other systems (e.g. association) but it is not
  "composed" of other products from a code generation perspective. It
  is logically, but not physically. This is in order to avoid the
  problems highlighted above.

* Classification of Model Transformation Approaches

- [[file:papers/Classification%20of%20Model%20Transformation%20Approaches.pdf][paper]]
- it seems what we are doing is in the spirit of frame processing. We
  need to read up on frame processing.
- we could explain our transforms from external models to
  code-generation model to augmented model with this "Why are
  model-to- model transformations needed? When bridging large
  abstraction gaps between PIMs and PSMs, it is easier to generate
  intermediate models rather than go straight to the target PSM."
- XDE seems to be interesting. Parameterisation is what we already do
  for facets. They seem to tackle patterns.

* Feature-Based Survey of Model Transformation Approaches

- [[file:papers/Feature-Based%20Survey%20of%20Model%20Transformation%20Approaches.pdf][paper]]
- we need to review dogen transformations with regards to the features
  in this paper and try to see where we can make the interfaces
  reflect this terminology. For example, we seem to loop through the
  model and then find elements of interest to mutate. They seem to
  suggest we should first query/filter the model using a rule and then
  apply the mutation to the result of the query/filter. This would
  perhaps improve the code. The query result could be a typed
  container (pointer container?) with the elements that match. That
  means we can then start to converge towards a rule engine. However,
  we'd have to go through all the transforms and see if they would all
  benefit from this split.
- we use imperative programming for the rules.
- We make use of both: "Transformations with source and target domains
  conforming to a single metamodel are referred to as endogenous or
  rephrasings; whereas transformations with different source and
  target metamodels are referred to as exogenous or
  translations."
- Dogen supports transformation tracing, which we called probing.
- we seem to use control parameters quite a lot. They are used to
  convey feature configuration. "Parameterization. The simplest kind
  of parameterization are control parameters, which allow passing
  values as control flags (see Figure 14). Control parameters are
  useful to implement policies. For example, a transformation from
  class models to relational schemas could have a control parameter
  specifying which of the alternative patterns of object-relational
  mapping should be used in a given execution."
- Dogen uses the mechanisms of the language to organise transforms
  into chains.
- not a big fan of the usage of the word rule.
- our source-target relationship is chosen based on the needs of the
  transform. Sometimes we use in-place, in other cases (such as
  merging and translation) we use distinct source and targets.
- we do not support incrementality by design. Makes the generator more
  complex.
- all of our transforms are unidirectional, with the exception of
  model merging. Here we need some kind of way of reading the
  protected region into the artefact.
- we extend the [[http://jamda.sourceforge.net/][Jamda]] approach:

#+begin_quote
Jamda is an open-source framework for building application generators
which create Java code from a model of the business domain. Instead of
a generator which produces one fixed architecture, Jamda provides a
structure and building blocks so that you can build an application
generator which does exactly what your project needs. It includes a
sample generator for J2EE applications which can either be tailored to
the needs of your J2EE project, or used as the basis of a generator
for a completely different architecture.

From a UML model of the application domain, a generator created with
Jamda could create the code for all the standard functions of
locating, displaying and updating the business objects in the
application. The developer would then concentrate on the
application-specific business logic, which is merged into the
generated application. In a typical application, the developer would
only need to write around 20% of the total system code.

Is it a Model Driven Architecture tool?

An application generator built using Jamda would perform the role of a
model compiler in the Object Management Group's Model Driven
Architecture specification. It takes a UML domain model as input, adds
new classes to the model to support the implementation, and then
generates executable code.
#+end_quote

- we follow exactly the same approach: "The openArchitectureWare
  Generator Framework propagates the idea of separating more complex
  source access logic, which might need to navigate and gather
  information from different places of the source model, from
  templates by moving the logic into user-defined operations of the
  source-model elements." We need to explain that our metamodel is
  designed with this in mind.
- we have chosen the direct manipulation approach for the M2M
  transforms:

#+begin_quote
Direct-Manipulation Approaches

These approaches offer an internal model representation plus some API
to manipulate it, such as JMI.  They are usually implemented as an
object-oriented framework, which may also provide some minimal
infrastructure to organize the transformations (e.g., abstract class
for transformations). However, users have to usually implement
transformation rules, scheduling, tracing, and other facilities,
mostly from scratch in a programming language such as Java.
#+end_quote

  However, we do provide the framework on which to implement the
  transforms so that users do not have to worry about any of the
  concerns listed above.
- actually perhaps we take an hybrid approach. 1) we use direct
  manipulation because the framework is defined in a GPL. 2) we also
  use the structure-driven approach because we have similar phasing:
  "Approaches in this category have two distinct phases: the first
  phase is concerned with creating the hierarchical structure of the
  target model, whereas the second phase sets the attributes and
  references in the target. The overall framework determines the
  scheduling and application strategy; users are only concerned with
  providing the transformation rules." 3) clearly we have an
  operational approach because we have more dedicated support for
  model transformation (e.g. the dogen API).
- our transforms are implemented as imperative rules: "A fully
  imperative rule (so-called procedure) has a name, a set of formal
  parameters, and an imperative block, but no patterns."
-

* Software Diversity: State of the Art and Perspectives

- [[file:papers/Software%20Diversity%20%E2%80%93%20State%20of%20the%20Art%20and%20Perspectives.pdf][paper]]
- they focus on "anticipating variability". We believe it should not
  be anticipated, but discovered and incorporated organically and
  systematically into the system.
- variability can either be planned or emergent. We believe in
  emergent variability. Another pillar of fractal systems
  development. Emergent does not imply implicit. You can make it
  explicit.
- the feature model of a general purpose, OO code generator lives in
  problem space - i.e. the problem space of code generation. This is a
  special case where the problem space and the solution space are the
  same. In fact "general purpose" is not quite right: it is special
  purpose in the sense its design to model only OO and features that
  can be expressed as functional dependencies of structural models.
- our approach seems to be this one: "annotative approaches or
  superimposed variants representing negative variability—all variants
  of the product line are included within the same model."
- the key advantage of our approach is that instead of having to do
  structural surgery to the model, we are simply switching facets on
  or off. This is a much easier thing to achieve. "Variant annotations
  define which parts of the model have to be removed to derive a
  concrete product model."
- its very important to split out the feature definition and the
  feature use. For definition we can rely on overloading UML
  (stereotypes, etc). However, we still need to figure out how to
  solve the contraints problem (dependencies, etc). For use we can
  simply rely on stereotypes and tagged values.
- orthogonal variability model seems interesting. We could go for
  capturing variability separate from the model, but variability
  application does require the model. Here we are referring to code
  generator variants.
- we use a compositional approach to associate data with model
  elements such as include files and metatype information.
- they use the term "model fragment" on compositional
  approaches. Perhaps proxy models are just model fragments.
- so we use positive variability at the model level and negative
  variability at the template level because facets are combined to
  generate the complete model representation.
- we seem to rely on both collaborative features (facets) as well as
  aspectual features (helpers).
- "Diversity interfaces and switches in Koala can be understood as
  concrete language constructs targeted at the implementation level to
  express variation points and associated variants."
- "When modeling variability, features or decisions are just (problem
  space) abstractions of the variability realized in real development
  artifacts." However, this is not the case with code generators
  because the problem space of the code generator is intertwined with
  the solution space.
- "Other variability modeling approaches define a separate artifact
  model, which exposes artifact abstractions to the decision or
  feature model". This seems to be what we are doing. We took
  advantage of the fact that code generators are a bit special and
  mapped the artefacts to the feature model (or vice-versa).
- we need to read the lose programming paper to see if that is what we
  are doing.
- feature interaction analysis is interesting but it is beyond the e
  scope of all the work we have outlined at present. Similarly with
  model checking and deductive logic.
- to some extend we are taking on the one thing approach (see
  paper). Its just that our end customers are not the SPL end
  customers but the code generator SPL end customers -
  i.e. developers. This means we put the developer at the centre of
  our concerns. In this light the one thing paper may be useful, most
  likely for the Dogen manual. Our objective is not to make the
  modeling of problem domain entities easier but to model solution
  space / implementation level patterns that appear in the artefacts.
- by controlling the entire pipeline we control the evolution and thus
  can confine it somewhat and make sure all the parts fit. The
  disadvantage is the limited developer pool to work on all the
  aspects of the tool chain.
- predictive vs opportunistic software reuse when using SPL

* Variability in Software Architecture: Current Practices and Challenges

- [[file:papers/Variability%20in%20Software%20Architecture:%20Current%20Practice%20and%20Challenges.pdf][paper]]
- it seems that the scope of variability is too vast. We need
  different strategies for different kinds of variability.
- interestingly, the "welcome mat" approach is also related to
  variability. That is, the support of multiple protocols (JSON, XML,
  HTTP, etc) for comms, the packaging as a library (static, shared),
  or as a component (COM, CORBA), or as a service, unbinding the UI
  technology from the implementation so that multiple UIs can be
  supported (Wt, GTK, Qt, TUI), etc are all variability concerns from
  an architectural perspective. The product team is responsible for
  enabling variability across most of these dimensions. What is
  lacking is a taxonomy/classification of the different kinds of
  variability.
- the paper provides a method for identifying variation points at the
  architectural level. It also provides an approach for analysing the
  VPs.
- functional core approach: define not just what varies in a product
  but also what is constant. The functional core is the essence of the
  product. We take this approach on Dogen. We need to define what we
  consider to be part of the functional core and what is allowed to
  vary. The functional core is what makes that product unique. Each
  functional core must be well-understood, and cannot be too large. It
  must be straightforward to decide if new functionality is in keeping
  with the functional core or not. Its understanding will evolve over
  time, incrementally.
- its not necessarily the case that everything needs to be
  automated/tooled around. Its useful to model variability separately
  even if its not possible to make the most of those models; it means
  everyone is conscious about introducing and documenting variability
  into the system. It should not be taken lightly.
- variability is a manifestation of indecision on a decision topic. If
  there is a decision such as "do not include" then we do not need a
  variation point. If there is a decision such "as always include"
  then its likely to be added to the functional core (although
  Savolainen disagrees and states that not all mandatory features are
  part of the functional core; not clear how so).

* Systems Variability Modeling: A Textual Model Mixing Class and Feature Concepts

- [[file:papers/Systems%20Variability%20Modeling:%20A%20Textual%20Model%20Mixing%20Class%20and%20Feature%20Concepts.pdf][paper]]
- definition of the feature model and the application (feature
  configuration) are different things. We should split them with two
  approaches: text based approach for the former, and model mixing for
  the latter.
- splitting features according to types is a good idea, but not sure
  we agree on their classification.

* A Common Metamodel for Code Generation
  :PROPERTIES:
  :ID:       7cf59c6b-3230-4805-8398-d9397711ac45
  :END:

- [[file:papers/A%20Common%20Metamodel%20for%20Code%20Generation.pdf][paper]]
- very similar to previous paper.
- We take a similar approach: "As has been outlined in [4], we use a
  package with common meta- modelling building blocks for all our
  metamodelling activities.  This makes it easier to speak about
  common concepts under the same name, much as Design Patterns in
  software engineering help programmers to talk about common
  programming concepts."
- they chose names similar to UML infrastructure; we have chosen names
  that are different to ensure there is no confusion between language
  concepts and modeling concepts, as well as not having problems with
  reserved words.
- they re-implemented UML infrastructure (parts of MOF that are in
  UML?), and question whether to reuse it or re-implement it. We take
  the view that whilst conceptually it is very useful, at the
  implementation level it does not add any value.
- there are differences between modeled languages (say namespaces /
  packages); it is the role of the metamodel to normalise them and the
  code generator to express them as idiomatically as possible.
- we use the same approach of named elements, but we partitioned the
  naming space.
- they do not constrain elements to exist in packages; we more or less
  do, although there are hacks to place things in the global
  namespace. We need to clean this up.
- due to normalisation (least common denominator) we also do not allow
  multiple inheritance on data structures.
- as the model is OO, there are limitations when modeling to non-OO
  languages such as C. It is still possible though, and useful even
  without support for operations.
- we do not distinguish between collections at the metamodel
  level. However, they link to aspects in the generator. At present we
  do not support built-in collections (e.g. int[]). This is only
  because the parsing engine does not parse them.
- we do not model entry points such as main, etc at present. We could
  in the future, but its better to simply bypass the dogen and create
  files manually.
- Java has package visibility, C# has internal visibility. Whilst we
  do not have a way to model these, we could create the notion of
  internal which in C++ means hide the header files and do not export
  symbols. In C# use external. In java, ignore it.

* A Code Generation Metamodel for ULF-Ware
  :PROPERTIES:
  :ID:       f80dfa29-68e9-4971-a183-3bbb8d81cee8
  :END:

- [[file:papers/A%20Code%20Generation%20Metamodel%20for%20ULF-Ware.pdf][paper]]
- extremely interesting paper, which tackles exactly the same problem
  as dogen: "While code can be generated from any model, we propose to
  use an intermediate model that is tailored to code generation
  instead. In order to be able to easily support different target
  languages, this model should be general enough; in order to support
  the whole process, the model has to contain behavioural as well as
  structural aspects."
- however, they seem to also focus on behaviour. Worth seeing what
  they have to say, but we are explicitly moving away from it.
- "Generally, adding another library with the same interface does not
  require new transformations." Similar to Dogen, except we also cater
  for impedance mismatches between metamodel and the library. For this
  we use an aspect oriented approach, where feature configuration is
  used to bind aspects to elements.
- they have decided to use MOF as their metamodel.  We have used UML
  to define our metamodel. We need a good explanation for our approach
  given its a lot less orthodox. We did not find the MOF types useful
  to our model. Need to justify this.
- crucial quote, totally applicable to us: "High-level models are
  quite different from programs in conventional programming
  languages. They abstract from most of the detail that a programming
  language exhibits. Once you want to generate real code, all this
  detail has to be filled in. This makes code generation from those
  models a difficult task. Moreover, many decisions in this process
  are similar for different target languages, but it is hard to make
  use of these commonalities." Our key objective was to design a model
  that modeled these aspects.
- very important point on "close to the language but not too close":
  "The reverse approach is to use models that are very low-level and
  close to a specific language. There have been a number of papers
  such as [6] implement- ing this. The metamodel obtained this way is
  close to the original BNF of the language, they are grammars in
  disguise. Models like this are difficult to obtain.  They would be
  the result of a model transformation from a high-level model.  Here,
  the intelligence would have to lie in the transformations."
- the crux of our approach is encapsulated in this passage: "How will
  this be represented in the model in a uniform fashion? One way is to
  have special metamodel-elements for print- ing text, and similarly
  for all the other library calls that differ; this also means changes
  to the metamodel if we want to include another call. The other way
  is to use a common runtime library that offers a uniform interface
  to the model and encapsulates differing functionality; clearly, this
  approach is superior."
- our approach is to make use of the standard libraries or third-party
  libraries available in languages but use templates and aspects as a
  way to fix the impedance mismatch. This saves us from having
  additional dependencies and makes programming more intuitive to the
  natives of a language. We can use the adoption paper as a rationale
  for why it is not a good idea to have a consistent API across
  languages.
- we need to explain why we wanted to support multiple
  languages. Reasons: 1) make sure we are not hard-coding the
  metamodel to just one language 2) expand the pool of contributors as
  much as possible.
- we created a level of abstraction from classes, modeling them
  according to the types: value objects, enumerations, etc.
- we do not support collections directly.
- we do not have a fixed type of primitives. We do split the notion of
  primitive and underlying primitive. This has to be explained in
  great detail. We do not have constraints on the presence of
  primitives, they are just handled like any other element.
- we do not support functions or operations.
- we use type mapping in order to create language agnostic models, and
  (will) support user overriding.
- our aims are also simple and readable code, using the language
  idioms at all times or high-level constructs such as patterns.
- our concept of system libraries and proxy libraries extends Piefel
  and Neumann's concepts of library interfaces. We need to explain this.

* Aspect-Oriented Model-Driven Software Product Line Engineering
  :PROPERTIES:
  :ID:       2184629c-a1de-4eed-b30d-9a0f3b951a76
  :END:

- [[file:papers/Aspect-Oriented%20Model-Driven%20Software%20Product%20Line%20Engineering.pdf][paper]]

** Expressing Variability in Structural Models

- reference to czarnecki's paper on superimposed variants: "In [13],
  the links are managed using stereotypes which requires invasive
  changes to the model that should be tailored." We need to add this
  to our list of disadvantages.
- our approach of splitting the generational model from the modeling
  model means that we do not have to worry about positive or negative
  variability in the modeling (structural) space (well, not entirely
  at any rate). Instead, it is (mostly) pushed down to the
  generation/templates level, where it is easier to handle. In a way,
  we disagree with Groher and Voelter's statements: "Variability can
  be described more concisely since in addition to the traditional
  mechanisms (e.g. patterns, frameworks, polymorphism), variability
  can be described on the more abstract level of models." Our
  generation model provides us a set of dimensions under which a model
  can vary so that handling variability at the implementation level
  becomes more manageable.
- we moved towards the world of facets for exactly the same reason
  that M2T has moved towards templates: it is the right mix of
  abstraction and hard-coding. Somethings are just too verbose to
  describe at the model level, such as serialisation. However, this
  may be an artefact of our main target language (C++), because on
  other languages such as C# we probably would rely more on aspects
  and less on facets.
- modeling space has a mapping from problem space to solution space. A
  solution space model in a different representation will have a one
  to one mapping to modeling space.
- we specifically remove the ability to handle positive/negative
  variability in the structural model to reduce complexity.
- conclusion: we have limited support for structural variability, by
  design. Or perhaps, we use structural variability only in one place,
  and that is going from frontend models into the modeling
  model. However, the variability is mainly concerned with type
  mapping rather than proper variability (positive/negative). This is
  still structural variability though since the modeling model will
  _vary_ according to the feature configuration selected by the user,
  which is conveyed via UML facilities: stereotypes and tagged values.
  Yet another take is that we use negative variability everywhere, but
  it does not have a structural manifestation due to the
  modeling/generation space split. In other words, users can switch
  off facets (negative variability) but since facets do not have a
  structural representation at the modeling level you cannot see
  it. But this is clearly negative variability.

** Expressing Variability in Model Transformations

- the code generation feature model is expressed as code, and can be
  queried by the model transformations. The model transformations are
  hard-coded to the generation model, and are designed to provide all
  the required queries. Extensions to queries requires changes to both
  the metamodel and the transformations.
- we only support one type of cross-metamodel transform: from frontend
  metamodel to modeling metamodel.
- most of our transforms are "in place transforms", called model
  modifications rather than model transformations; that is, the
  original model is modified. A few transforms such as merging do not
  touch the inputs, so these are proper transforms. We need to explain
  that we treat modifications and transforms the same way.
- we code-generate the feature model, but every time a new feature is
  added we need to manually update the affected transforms. This is
  done to keep code generation easy to understand. Note that this is
  feasible because this is not a general purpose code generator but a
  special purpose code generator.

** Expressing Variability in Code Generation Templates

- As with transformation, we access the feature model directly and
  changes must be done manually.
- we use stitch and stitch2 to generate regular c++ code that is human
  readable, debuggable etc. Developers do not need to learn a new
  toolset.
- we use AO for handling variability just for one case: helpers. The
  objective is to reduce the amount of third party code needed to
  solve the impedance mismatch between generated code and supporting
  libraries. The rest is handled by hard-coding the code generator
  feature model into the templates. This is done by design.
- we could use "feature casting": create a base class for features (or
  feature groups, check terminology). Users request the feature (or
  group) via its path, and the template function takes a type
  parameter for the casting. It could return an optional, if the
  feature is optional.Annoyingly this is not required for "core"
  features because we can see the type definition, but if we use this
  pattern everywhere, it means users can add their own features,
  register them, and consume them from their templates.

** Expressing Variability in Code

- mainly by use of apsects and integrating them with the feature
  model. Use of comments allows code generator to remove code that is
  not required. Interestingly, this could be done via our proposed
  merging approach: protected regions could be annotated with
  additional attributes, and these could be linked to features. During
  generation we could not express the protected regions for which the
  features have not been switched on. However, we do not have a use
  case for this. Also its not clear where the protected regions would
  be stored - the code generation product line vs software product
  line split is a bit confusing.

** Home Automation Case Study

- it is still not clear why we need to resort to DSLs in order to
  create unique products when we could just as well treat each product
  in a product line as data and have rules for specific
  configurations. This seems to be the case with the home automation.
- in addition, the PIM to PSM transformation, could also be thought of
  as parameterisation in variability space. That is, we could simply
  have a set of mappings that determine platform specific
  implementations (component technology, programming language, etc)
  which are chosen by the user (via feature configuration). Then, we
  go from PIM directly to code. This simplistic approach is more
  adequate to new MDE users. The PIM to PSM options are canned, but
  extensible, the hard-way: add your own plugin, provide a new facet,
  select it on your feature configuration.
- orthogonal variability: variability which is not dependent on to the
  problem space model instance - e.g. applies regardless of the
  specific configuration. "By orthogonal we mean variability that
  affects multiple domain entities and their subsequent processing
  (i.e. transformation) steps."
- reflection layer seems to be useful to generate a GUI at
  runtime. However, it seems easier to map GUI elements at generation
  time, and then determine which dialogues are available in the UI
  depending on the presence of elements of the problem space model.
- configurative variability: this seems to be the technical term for
  using data to manage variability. Our approach is to push a lot of
  structural variability into configurative variability, so that the
  end users of the system need to deal with it rather than the
  developers. Actually, configurative variability can be realised by
  structural variability.
- "The higher the abstraction level is, the fewer variation points
  exist. Features expressed on models level are thus inherently
  simpler than features expressed on code generation level. We
  therefore argue to always express features on the highest possible
  level."

** Conclusions

- "The higher the abstraction level is, the fewer variation points
  exist. Features expressed on models level are thus inherently
  simpler than features expressed on code generation level. We
  therefore argue to always express features on the highest possible
  level." - however, there is still a trade-off: by expressing
  features in the model and producing structural transformations, we
  then push the complexity to the transforms and templates. We should
  distinguish between features that have a good structural
  representation from features that are trivial functions of structure
  such as serialisation, test data generation, etc. For these, it
  makes more sense not to represent them structurally but instead have
  the variation points expressed at the template level.
- this raises an interesting point:

* An Aspect-Oriented and Model-Driven Approach for Managing Dynamic Variability
  :PROPERTIES:
  :ID:       643b1ad0-e48b-4b3a-a205-64af900378f8
  :END:

- [[file:papers/An%20Aspect-Oriented%20and%20Model-Driven%20Approach%20for%20Managing%20Dynamic%20Variability.pdf][paper]]
- describes a framework to dynamically generate valid
  configurations.
- uses a metamodel to describe AOM, allowing the application of
  aspects dynamically to a model (called SMART ADAPTERS)
- computes diff and match models and then based on that outputs a
  configuration. The configuration can then be validated before its
  applied. All of this is done at run time so for systems where
  configuration transitions need to be quick (or predictable), this is
  not suitable out of the box. They suggest pre-computing critical
  configurations up front (at startup?)
- does not seem to be directly applicable to Dogen: the code generator
  itself does not have a need for "discovery". Applications built with
  it may have such a need, but its not obvious creating a reflection
  based mechanism, plus AOM etc is the most straightforward way of
  handling this. We need a specific use case in order to frame this
  paper.
- in general, we do not have a requirement for dynamic
  variability. Well, we do but it is handled via plugins etc. We have
  a limited requirement for this. Our aim is to reduce the variability
  surface area so that we reduce the complexity associated with
  handling it.

* A Feature Model for Model-to-Text Transformation Languages
  :PROPERTIES:
  :ID:       10533850-b1f4-497a-ad11-dee38d3d7f15
  :END:

** Abstract

- it seems like the main contribution of this paper is a feature model
  for M2T languages. This is useful so we can justify out choices in
  Stitch2.

** Introduction

- migrating M2T languages is very difficult. Effectively its a form of
  vendor lock-in. Its easier to just rewrite than to port.
- most M2T languages are template based. There are other types though
  (visitor pattern based, explicit printing of statements). Paper only
  focuses on template based.
- their paper helps users that want to select a M2T language.

** A Feature Model for M2T Languages

- Their work is largely based on Czarnecki and Helsen's, but they
  considered template based M2T a degenerate case whereas Rose etal
  consider it the most significant case.
- they use FeatureIDE to draw their feature diagrams. It is eclipse
  based.
- transformation style: imperative or declarative.
- templates: direction of escaping (is the code escaped or is the
  template escaped), typed templating language, textual templating
  language.
- output: destination of the output - normally to file?
  post-processing: clang format, protected region handling.
- modularity mechanisms: template reuse. Here we provide only a single
  way of reusing a template, the aspect approach. The idea is that for
  specific types we may need to inject additional functionality. This
  is not meant as a generic reuse mechanism, instead for this users
  should use copy & paste.
- tracing: how to relate the template to the generated code. Note that
  we are not talking about the transformation from template into C++
  code capable of generating a file, but the end file. Its as if we
  need two levels of tracing here.
- incrementality: the way the transformation is executed in response
  to changes to the source model. Ideally we should have a separation
  between the internal generation of the code and the updating of
  files in the file system.
- directionality: normally uni-directional, e.g. from model to text.
- tools: not clear, does not include IDE with syntax highlighting and
  debugging.

* Modeling Variability in Template-based Code Generators for Product Line Engineering
  :PROPERTIES:
  :ID:       d5135a6d-14b8-4e8e-9c84-e1461cdddf3f
  :END:

- [[file:papers/Modeling%20variability%20in%20template-based%20code%20generators%20for%20product%20line%20engineering.pdf][paper]]
- how to model variability using Variability Regions instead of Aspect
  Oriented Programing.

** Abstract

- we need to define what is meant by variant.
- what is meant by "adaptable and extensible"? What are the dimensions
  under which we "adapt and extend"?
- authors are critical, but being fixed to a set of features is not
  necessarily a bad thing, if there is good support out of the box.
- the key point seems to be a mapping of features to Variability
  Regions (VRs), and having dependency management amongst VRs.
- we need a good discussion on why VRs are better than aspects, and
  how they relate.

** Introduction

- paper is critical of monolithic code generators, but there are
  advantages; they are a lot simpler. Also, its important to think of
  reuse as a dimensional space. We can facilitate some kinds of reuse
  whilst precluding others in the search for lowering complexity.
- interestingly, they seem to imply that both Acceleo and Xtend have
  an approach of handling variability which relies on "language
  specific approaches for implementing variability, e.g. design
  patterns." This ties in with the [[file:papers/Handling%20Variability%20in%20Model%20Transformations%20and%20Generators.pdf][Voelter paper]].
- the paper focuses on Code Generator Product Lines, designed
  specifically to handle variability in code generators. Their
  approach is based on Variability Regions and they state that these
  can be applied to any code generator.
- layers are an extension of the feature oriented programming (FOP)
  concept, applied to code generation templates. They define how
  parts of a template or an entire template can be resued within
  layers. The key point is to decouple the concepts from the template
  technology, so that they can be applied to any code generator. This
  seems extremely useful for Dogen.

** Variability Concepts in Code Generator Product Lines

- we are creating a Code Generator Product Line (CGPL). This is quite
  confusing. Basically we do not give end users a code generator; we
  give them a "template" with which to create specific code generators
  for their use case. The customisation of the code generator is done
  via the variability mechanisms we make available to them, which at
  present are:
  - structural: object templates, stereotypes such as visitor,
    enumeration etc.
  - non-structural: all of the dogen machinery to customise a model
    such as adding licences, comments on or off (not yet implemented),
    enabling of facets, etc.  When we finally implement "profiles" on
    top of stereotypes, this will also be a non-structural mechanism,
    even though it spans structure (users define classes, which are
    associated with stereotype (the class name) and toggle features
    on/off).
- a concrete code generator product is called a Variant. A CGPL is
  like a Software Product Line (SPL) factory, because it generates
  concrete code generators. These then generate SPLs?
- FOP is an approach to implement SPLs that is based on building
  software systems by composing features. A feature represents a
  configurable unit of a software system that represents a
  requirement-satisfying design decision. Features are arranged in
  layers that contain artefacts. These artefacts appear to be
  templates. Artefacts may refine multiple other artefacts. However,
  it seems the paper puts java classes and layers on the same
  plane. It seems they make the distinction between "code generated
  artefacts" and others. Code generator layers are used for code
  generator artefacts.
- also, its not obvious why we need both VRs and layers. We could
  simply say that a template is made up of a set of VRs, and the VRs
  are switched on or off based on feature configuration. This is
  slightly better than AOP because we don't need all of the AOP
  machinery (cross-cutting concerns, joint points, etc).
- if we map this model to Dogen, a stitch template becomes an
  artefact, and helpers then provide a way to "compose"
  templates. Facets "map" the feature (e.g. boost serialisation) to
  the artefact (e.g. template implementing boost serialisation). We
  have transported the feature configuration into the code generation
  metamodel and the assistant by adding generation specific types to
  it: =requires_hashing_helper= etc. The key problem is that in Dogen
  we do not have a concept of Variability Regions nor will it be easy
  to implement because it would require dramatic changes to
  stitch. Also the "conditions" for each VR can be quite complex. Its
  not clear what advantages we'd gain from VRs in Dogen. The
  addressability of VRs is quite interesting, but not clear how we'd
  make use of it to improve things. We could possibly make a
  description of the model of the template as a set of named
  variability regions with associated predicates, but then explain
  that implementation-wise, its more sensible to code it by hand given
  the target audience.
- we need to have a section on "informal reuse" / copy and paste reuse.
- we need to do a write up of why we think that the increase in
  complexity by having a layer/feature DSL is not a good fit for an
  entry-level code generator product line. We need to make the case
  for keeping templates quite close to regular code.
- interesting paper: [[https://link.springer.com/book/10.1007%252F978-3-642-37521-7][Feature-Oriented Software Product Lines: Concepts
  and Implementation]]. However seems to be behind a paywall.
- commands in layer DSL: define a layer, VR replaces other VR, appears
  before or after a VR.

** Code Generator Variant Configuration and Generation

- a CGPL consists of a number of layers, and each layer contains a
  number of templates.
- their approach works more like a library, where concrete CGPL reuse
  the basic infrastructure to create the specific CGPL the user is
  interested in. This allows for both a high degree of reuse and a
  high degree of configurability. The downside is complexity, because
  the user needs to understand two DSLs.
- Layer Definition Language (LDL) is the DSL created to model the
  layer operations.
- variant configuration: we need to ensure the set of layers generates
  a valid configuration. They use a coloured graph to do this. The
  graph must be acyclic, and they do not support multiple
  inheritance. We could probably use a similar approach for features.
- in their case: "based on the layer definition model and the product
  configuration model, a concrete code generation variant is created."
  In our case, this is done at dogen run time. Code generator variants
  are a function of the product configuration model which is embedded
  in the structural model. Variation is constrained in order to
  achieve this. Composition / layering is achieved not at the template
  level but at the facet level. Actually perhaps what we are saying is
  that facets are the layers and we explicitly move them away from the
  template / file level to tame complexity. We make use of a fixed set
  of layers but we use them at a higher conceptual level. Variability
  Regions are no longer required at the layer level - we vary by the
  entire layer at this level. We can then make an argument for a) why
  layers at the artefact level is the wrong approach if you are
  optimising to reduce complexity b) the downside of losing some
  object orientation by moving responsibility across to other classes,
  not a problem in hybrid languages like C++ or procedural languages
  like C but a problem in Java/C#.
- in this case we can then say we use exactly the same approach for
  the creation of concrete variants (e.g. coloured graph). We need to
  also add how we allow variation at the metamodel element instance
  level.
- transitivity is an issue for layers as is an issue for facets. We
  need to reuse their approach.
- the run time of the code generator is called generation time. All
  our operations are executed at generation time. Perhaps we should
  name our variants generation time variants.
- we need to make a case for stitch as a template language very close
  to a general purpose language. We need to compare it to Xpand.

** Demonstrating Example for Variability Regions

- terminology: we describe templates as "archetypes" whereas they seem
  to call them artefacts (or somehow artefacts are related to
  templates). In our model, empty VRs do not make sense nor do empty
  layers. We have multiple archetypes to express a given
  metatype. These form layers (e.g. what we called facets).
- the problem with using comments to mark VRs are mentioned, and they
  seem to be exactly the same as with protected regions.
- Xpand's way of allowing method definitions "out of order" makes
  templates much harder to read - e.g. to visualise what the output of
  the template will be.

** Industrial Case Study

- generation of inner classes: this can be handled easily by metadata
  and using namespaced names on the structural representation.
- classes tagged with a stereotype instead of generating regular
  fields, generate "special" fields.
- suffixes added to class names: this should be a metadata parameter
  to the code generator.
- their research questions are very good, and very measurable. The
  approach of doing "old way" and "new way" and comparing the results
  is very effective. Its not so easy for us to do this, but if we
  could it would be a major selling point.
- we need to somehow state that our approach allows a large number of
  CGPL to be generated via generation time configuration without any
  need to copy & paste reuse nor the complexity of a general purpose
  VR mechanism. If we could find a project that is already using say
  EMF and prove that with just our code generator out of the box (plus
  minor alterations) we could generate the same code we would have
  "proof".

** Related Work

- here they provide a taxonomy of approaches to express variability in
  the solution space. We need to classify our approach against these:
  - annotative: specify all variants in one model.
  - compositional: combine different model fragments to derive a
    specific variant.
  - delta modeling: applies transformations to a core model
- these approaches seem to overlap with Positive and Negative
  variability.
- dogen seems to use a combination of annotational (metadata tags) and
  compositional (profiles are defined separately). Once we move
  profiles into the models, these become annotational.
- seems important to have the concept of VRs at the template level.
- we seemed to have followed the Groher approach by handling all
  structural differences on model level by the transformation
  layer. We should discuss this.
- we explicitly do not allow dynamic overriding of parts of a
  template, but allow users to override an entire layer (e.g. facet)
  with their own implementation located somewhere else in generation
  space (actually it could also be in the same location in space, but
  we need some handling to give higher priority of user templates over
  system templates). In this sense we have copied the acceleo
  approach, but we do so by generating a DLL and injecting it to the
  code generator. This generates further CGPLs. Users can create
  templates via the copy & paste reuse. However, we strongly suggest
  this as the last case scenario, when a user has very specific
  requirements which cannot be reconciled with the majority of the
  user base.
